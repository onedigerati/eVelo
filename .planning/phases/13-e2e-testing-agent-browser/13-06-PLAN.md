---
phase: 13-e2e-testing-agent-browser
plan: 06
type: execute
wave: 3
depends_on: ["13-02", "13-03", "13-04"]
files_modified:
  - .github/workflows/e2e.yml
  - test/e2e/run-all.js
autonomous: true

must_haves:
  truths:
    - "GitHub Actions workflow runs E2E tests on PR and push"
    - "Test orchestrator runs smoke, workflow, responsive tests in sequence"
    - "CI fails if any E2E test fails"
    - "Screenshots uploaded as artifacts on failure for debugging"
  artifacts:
    - path: ".github/workflows/e2e.yml"
      provides: "GitHub Actions E2E test workflow"
      min_lines: 40
    - path: "test/e2e/run-all.js"
      provides: "Test orchestrator that runs all E2E tests"
      min_lines: 60
  key_links:
    - from: ".github/workflows/e2e.yml"
      to: "test/e2e/run-all.js"
      via: "npm run test:e2e"
      pattern: "npm run test:e2e"
    - from: ".github/workflows/e2e.yml"
      to: "agent-browser"
      via: "postinstall"
      pattern: "npm ci"
---

<objective>
Create GitHub Actions CI/CD integration for running E2E tests on pull requests and pushes.

Purpose: Automate E2E test execution to catch regressions before merge. Ensures all tests pass before code reaches main branch.
Output: GitHub Actions workflow that runs smoke, workflow, and responsive tests. Test orchestrator script for local and CI execution.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/13-e2e-testing-agent-browser/13-RESEARCH.md
@.planning/phases/13-e2e-testing-agent-browser/13-01-SUMMARY.md
@.planning/phases/13-e2e-testing-agent-browser/13-02-SUMMARY.md
@.planning/phases/13-e2e-testing-agent-browser/13-03-SUMMARY.md
@.planning/phases/13-e2e-testing-agent-browser/13-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test orchestrator script</name>
  <files>test/e2e/run-all.js</files>
  <action>
  Create a test orchestrator that runs all E2E tests in sequence:

  ```javascript
  // test/e2e/run-all.js
  // E2E test orchestrator - runs all tests in sequence
  import spawn from 'cross-spawn';
  import { fileURLToPath } from 'url';
  import { dirname, join } from 'path';

  const __filename = fileURLToPath(import.meta.url);
  const __dirname = dirname(__filename);

  // Tests to run (in order)
  const TESTS = [
    { name: 'Smoke Test', script: 'smoke.js', critical: true },
    { name: 'Workflow Test', script: 'workflow.js', critical: true },
    { name: 'Responsive Test', script: 'responsive.js', critical: false },
    // Note: charts.js excluded from CI - requires baseline capture first
    // Run manually with: npm run test:e2e:charts
  ];

  /**
   * Run a single test script
   * @param {string} name - Test name for display
   * @param {string} script - Script filename
   * @returns {Promise<boolean>} - Whether test passed
   */
  async function runTest(name, script) {
    return new Promise((resolve) => {
      console.log(`\n${'='.repeat(60)}`);
      console.log(`Running: ${name}`);
      console.log('='.repeat(60));

      const testPath = join(__dirname, script);

      // Use cross-spawn for Windows compatibility
      const proc = spawn('node', [testPath], {
        stdio: 'inherit',
        cwd: process.cwd()
      });

      proc.on('close', (code) => {
        if (code === 0) {
          console.log(`\n[PASS] ${name} completed successfully`);
          resolve(true);
        } else {
          console.log(`\n[FAIL] ${name} failed with exit code ${code}`);
          resolve(false);
        }
      });

      proc.on('error', (err) => {
        console.log(`\n[ERROR] ${name} failed to start: ${err.message}`);
        resolve(false);
      });
    });
  }

  /**
   * Run all E2E tests
   */
  async function runAllTests() {
    console.log('============================================================');
    console.log('eVelo E2E Test Suite');
    console.log('============================================================');
    console.log(`Running ${TESTS.length} tests...\n`);

    const startTime = Date.now();
    const results = [];
    let criticalFailure = false;

    for (const test of TESTS) {
      const passed = await runTest(test.name, test.script);
      results.push({ name: test.name, passed, critical: test.critical });

      if (!passed && test.critical) {
        criticalFailure = true;
        // Continue running other tests to get full report
      }
    }

    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(1);

    // Print summary
    console.log('\n');
    console.log('============================================================');
    console.log('TEST SUMMARY');
    console.log('============================================================');

    for (const result of results) {
      const status = result.passed ? '[PASS]' : '[FAIL]';
      const critical = result.critical ? ' (critical)' : '';
      console.log(`${status} ${result.name}${critical}`);
    }

    const passCount = results.filter(r => r.passed).length;
    const failCount = results.filter(r => !r.passed).length;

    console.log('------------------------------------------------------------');
    console.log(`Total: ${passCount} passed, ${failCount} failed`);
    console.log(`Duration: ${duration}s`);
    console.log('============================================================');

    // Return success if no critical failures
    // (non-critical tests like responsive can fail without blocking CI)
    if (criticalFailure) {
      console.log('\nCritical test(s) failed - CI should fail');
      return false;
    } else if (failCount > 0) {
      console.log('\nNon-critical test(s) failed - CI passes with warnings');
      return true;  // Still pass CI
    } else {
      console.log('\nAll tests passed!');
      return true;
    }
  }

  // Run tests
  runAllTests()
    .then(success => process.exit(success ? 0 : 1))
    .catch(err => {
      console.error('Test suite error:', err);
      process.exit(1);
    });
  ```

  Key design decisions:
  - Run tests sequentially (not parallel) to avoid server port conflicts
  - Continue running all tests even if one fails (for full report)
  - Mark tests as critical or non-critical
  - Critical failures cause CI to fail
  - Non-critical failures log warnings but pass CI
  - Uses cross-spawn for Windows compatibility
  - Exclude charts.js from CI (requires baseline capture first)
  </action>
  <verify>
  - File exists at test/e2e/run-all.js
  - No syntax errors: node --check test/e2e/run-all.js
  - Uses cross-spawn (grep 'cross-spawn' test/e2e/run-all.js)
  - Runs smoke.js, workflow.js, responsive.js
  </verify>
  <done>Test orchestrator script created that runs all E2E tests</done>
</task>

<task type="auto">
  <name>Task 2: Create GitHub Actions workflow</name>
  <files>.github/workflows/e2e.yml</files>
  <action>
  Create GitHub Actions workflow for E2E tests:

  ```yaml
  # .github/workflows/e2e.yml
  # E2E test workflow using agent-browser
  name: E2E Tests

  on:
    pull_request:
      branches: [main, master]
    push:
      branches: [main, master]
    workflow_dispatch:  # Allow manual trigger

  jobs:
    e2e:
      name: Run E2E Tests
      runs-on: ubuntu-latest
      timeout-minutes: 15

      steps:
        - name: Checkout
          uses: actions/checkout@v4

        - name: Setup Node.js
          uses: actions/setup-node@v4
          with:
            node-version: '20'
            cache: 'npm'

        - name: Install dependencies
          run: npm ci
          # Note: postinstall script runs agent-browser install automatically

        - name: Verify agent-browser installation
          run: npx agent-browser --version || echo "agent-browser version check failed"

        - name: Run E2E tests
          run: npm run test:e2e
          env:
            CI: true
            # Increase Node memory for simulation
            NODE_OPTIONS: --max-old-space-size=4096

        - name: Upload screenshots on failure
          if: failure()
          uses: actions/upload-artifact@v4
          with:
            name: e2e-screenshots
            path: |
              test/e2e/screenshots/current/
              test/e2e/screenshots/diff/
            retention-days: 7
            if-no-files-found: ignore

        - name: Upload test logs on failure
          if: failure()
          uses: actions/upload-artifact@v4
          with:
            name: e2e-logs
            path: |
              *.log
              test/e2e/*.log
            retention-days: 7
            if-no-files-found: ignore
  ```

  Key considerations:
  - Use ubuntu-latest (agent-browser has Chromium for Linux)
  - Cache npm dependencies for faster runs
  - postinstall script runs agent-browser install automatically
  - Upload screenshots AND diff images as artifacts on failure
  - 15-minute timeout (simulations can take time)
  - Increase Node memory for Monte Carlo simulation
  </action>
  <verify>
  - File exists at .github/workflows/e2e.yml
  - YAML syntax is valid (no parse errors)
  - Triggers on pull_request and push to main/master
  - Runs npm ci (not npm install)
  - Uploads artifacts on failure
  </verify>
  <done>GitHub Actions workflow created for E2E tests</done>
</task>

<task type="auto">
  <name>Task 3: Ensure .github/workflows directory exists</name>
  <files>.github/workflows/.gitkeep</files>
  <action>
  Ensure the .github/workflows directory exists for the workflow file:

  1. Check if .github/workflows/ directory exists
  2. If not, create it with mkdir -p
  3. The e2e.yml file from Task 2 will be placed here

  This is typically automatic when writing the workflow file, but explicitly verify.
  </action>
  <verify>
  - Directory exists: .github/workflows/
  - e2e.yml file is in .github/workflows/
  </verify>
  <done>.github/workflows directory verified/created</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. test/e2e/run-all.js exists and runs all tests sequentially
2. .github/workflows/e2e.yml is valid YAML
3. Workflow triggers on PR and push to main/master
4. Workflow uses npm ci (not npm install)
5. Workflow uploads screenshot and diff artifacts on failure
6. Test orchestrator distinguishes critical vs non-critical tests
</verification>

<success_criteria>
1. run-all.js executes smoke, workflow, responsive tests in sequence
2. run-all.js marks smoke and workflow as critical, responsive as non-critical
3. run-all.js prints summary with pass/fail counts and duration
4. GitHub Actions workflow runs on pull_request and push
5. Workflow installs Node.js 20 and runs npm ci (triggers postinstall)
6. Workflow uploads screenshot artifacts on failure (both current/ and diff/)
7. Exit code 0 when critical tests pass, non-zero when critical tests fail
</success_criteria>

<output>
After completion, create `.planning/phases/13-e2e-testing-agent-browser/13-06-SUMMARY.md`
</output>
